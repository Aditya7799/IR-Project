{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import codecs\n",
    "import io\n",
    "import gensim\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import pickle\n",
    "from sklearn import neighbors\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection  import GridSearchCV\n",
    "from sklearn.model_selection  import RandomizedSearchCV\n",
    "from time import time\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import sklearn.metrics\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import diags\n",
    "\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "import pickle\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS-SS Similartiy Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_SS:\n",
    "    def Cosine(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return np.dot(vec1, vec2.T)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def VectorSize(self, vec: np.ndarray):\n",
    "        return np.linalg.norm(vec)\n",
    "\n",
    "    def Euclidean(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return np.linalg.norm(vec1-vec2)\n",
    "\n",
    "    def Theta(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return np.arccos(self.Cosine(vec1, vec2)) + np.radians(10)\n",
    "\n",
    "    def Triangle(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        theta = np.radians(self.Theta(vec1, vec2))\n",
    "        return (self.VectorSize(vec1) * self.VectorSize(vec2) * np.sin(theta))/2\n",
    "\n",
    "    def Magnitude_Difference(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return abs(self.VectorSize(vec1) - self.VectorSize(vec2))\n",
    "\n",
    "    def Sector(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        ED = self.Euclidean(vec1, vec2)\n",
    "        MD = self.Magnitude_Difference(vec1, vec2)\n",
    "        theta = self.Theta(vec1, vec2)\n",
    "        return math.pi * (ED + MD)**2 * theta/360\n",
    "\n",
    "\n",
    "    def __call__(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return self.Triangle(vec1, vec2) * self.Sector(vec1, vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Vocabulary and W2W Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructAndSave(filelist,fname,isCosine):\n",
    "    if(os.path.isfile(\"preprocessedFiles/\"+fname+ '.pickle')):\n",
    "        with open(\"preprocessedFiles/\"+fname+ '.pickle','rb') as handle:\n",
    "            cdict = pickle.load(handle)\n",
    "    else:\n",
    "        cdict = constructDict(filelist)\n",
    "        print(cdict)\n",
    "        with open(\"preprocessedFiles/\"+fname+ '.pickle', 'wb') as handle:\n",
    "            pickle.dump(cdict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    computeWordToWordMatrix(cdict,fname,isCosine)\n",
    "\n",
    "\n",
    "def removeWordswithFreq_lessThanK(wordDict,freq,K):\n",
    "    words = [k for k,v in freq.items() if v <= K]\n",
    "    removeStopWords(wordDict,words)\n",
    "\n",
    "def constructDict(fileList):\n",
    "    ps = PorterStemmer()\n",
    "    l = WordNetLemmatizer()\n",
    "    freq = defaultdict(int)\n",
    "    wordDict = corpora.dictionary.Dictionary()\n",
    "    for file in fileList:\n",
    "        doc = tokenizeFile(file)\n",
    "        for x in doc:\n",
    "            freq[ps.stem(x)]+=1\n",
    "        wordDict.add_documents([[ps.stem(x)for x in doc]])\n",
    "    stopWords = getStopWordList(\"../stopwords\")\n",
    "    removeStopWords(wordDict,stopWords)\n",
    "    removeWordswithFreq_lessThanK(wordDict,freq,5)\n",
    "    return wordDict\n",
    "\n",
    "\n",
    "def computeWordToWordMatrix(myDict, fname,isCosine):\n",
    "    TS_SS_object = TS_SS()\n",
    "    wordVectorModel = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    wordToWord = np.eye((len(myDict)))\n",
    "    for i in np.arange(len(myDict)): #(i,w) in myDict.items():\n",
    "        try:\n",
    "            #wx=wordVectorModel[myDict[i]]\n",
    "            for j in np.arange(i): #(ii,ww) in myDict.items():\n",
    "                try:\n",
    "                    #wwx=wordVectorModel[myDict[j]]\n",
    "                    if(isCosine):\n",
    "                        wordToWord[i,j] =  wordVectorModel.similarity(myDict[i],myDict[j])#np.dot(wx,wwx)\n",
    "                    else:\n",
    "                        wordToWord[i,j] = TS_SS_object(wordVectorModel[myDict[i]] , wordVectorModel[myDict[j]])\n",
    "                except KeyError:\n",
    "                    None\n",
    "                    #nothing\n",
    "        except KeyError:\n",
    "            wordToWord[i,i] = 1\n",
    "\n",
    "    a_tril = np.tril(wordToWord, k=0)\n",
    "    a_diag = np.diag(np.diag(wordToWord))\n",
    "    wordToWord = a_tril + a_tril.T - a_diag\n",
    "    \n",
    "    if(isCosine):\n",
    "        fname+=\"_Cosine\"\n",
    "    else:\n",
    "        fname+=\"_TSSS\"\n",
    "    \n",
    "    np.save(\"preprocessedFiles/\"+fname,wordToWord)\n",
    "#     with open(\"preprocessedFiles/\"+fname+ '.pickle', 'wb') as handle:\n",
    "#         pickle.dump(myDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return wordToWord\n",
    "def computeW2W(w2w,fileV):\n",
    "\treturn fileV.dot( w2w) #np.dot(fileV,w2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStopWordList(pathToStopWords):\n",
    "    # with open(pathToStopWords) as f:\n",
    "    #     stopwords = [word.rstrip(\"\\n \").decode(\"iso-8859-1\") for word in f if len(word.rstrip(\"\\n \"))>0]\n",
    "    StopWords = stopwords.words('english')\n",
    "    return StopWords\n",
    "\n",
    "\n",
    "def tokenizeFile(filepath):\n",
    "    # with open(filepath) as f:\n",
    "    with codecs.open(filepath, encoding='utf-8',errors='ignore') as f:\n",
    "        words = [(re.sub('[^a-zA-Z\\']', ' ', line)).lower().split() for line in f]\n",
    "\n",
    "        #print words\n",
    "    #words.flatten()\n",
    "    # words = [val for sublist in words for val in sublist]\n",
    "    words = filter(isAlphanumeric,[val for sublist in words for val in sublist])\n",
    "    # words = [val.decode(\"iso-8859-1\") for sublist in words for val in sublist]\n",
    "\n",
    "    return list(words)\n",
    "\n",
    "def isAlphanumeric(text):\n",
    "    return text.isalnum()\n",
    "\n",
    "def removeStopWords(wordDict, stopWords):\n",
    "    toRemove = []\n",
    "    for (i,w) in wordDict.items():\n",
    "        if w in stopWords:\n",
    "            toRemove.append(i)\n",
    "    wordDict.filter_tokens(bad_ids=toRemove)\n",
    "    wordDict.compactify()\n",
    "\n",
    "def loadDataSet(path):\n",
    "    counter = 1;\n",
    "    filesAll = []\n",
    "    labelsAll = np.zeros(0)\n",
    "    for dirName in filter(os.path.isdir, [path + \"/\" + p for p in os.listdir(path)]):\n",
    "        #print dirName\n",
    "        files = [dirName + \"/\" +filename for filename in os.listdir(dirName)]\n",
    "        labels = np.zeros(len(files))\n",
    "        labels[:] = counter\n",
    "        filesAll.extend(files)\n",
    "        labelsAll = np.append(labelsAll,labels)\n",
    "        counter += 1\n",
    "    return filesAll,labelsAll\n",
    "\n",
    "#load data with multiple dataset\n",
    "def loadDataWithPreDefTest(pathTrain,pathTest):\n",
    "    fileTrain, labelTrain = loadDataSet(pathTrain)\n",
    "    fileTest, labeltest = loadDataSet(pathTest)\n",
    "\n",
    "    return fileTrain, labelTrain, fileTest, labeltest\n",
    "\n",
    "\n",
    "#return the train and test concateneted, and the index where the test set starts\n",
    "def loadDataWithTest(pathTrain,pathTest):\n",
    "    fileTrain, labelTrain = loadDataSet(pathTrain)\n",
    "    fileTest, labelTest = loadDataSet(pathTest)\n",
    "    files = fileTrain + fileTest\n",
    "    labels = np.append(labelTrain,labelTest)\n",
    "    #print len(labels)\n",
    "    return files, labels, len(fileTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allmeasures(preds, gt, pp):\n",
    "    n_classes = len(np.unique(gt))\n",
    "    gt2 = np.zeros((len(gt), n_classes))\n",
    "    #print \"Uniks\", np.unique(gt)\n",
    "    for i, elem in enumerate(gt):\n",
    "        gt2[i, int(elem-1)] = 1\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(gt2[:, i], pp[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(gt2.ravel(), pp.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    rocmicro =  roc_auc[\"micro\"]\n",
    "    rocmacro = roc_auc[\"macro\"]\n",
    "    f1macro =  sklearn.metrics.f1_score(gt, preds, average=\"macro\")\n",
    "    f1micro =  sklearn.metrics.f1_score(gt, preds, average=\"micro\")\n",
    "    acc = sklearn.metrics.accuracy_score(gt, preds)\n",
    "\n",
    "    return rocmicro, rocmacro, f1micro, f1macro, acc\n",
    "def computeBOW(files,cDict):\n",
    "    BOW = np.zeros((len(files),len(cDict)))\n",
    "    c=0\n",
    "    for f in files:\n",
    "        BOW[c,:] = densify(cDict.doc2bow(tokenizeFile(f)),len(cDict))\n",
    "        c+=1\n",
    "    return BOW\n",
    "\n",
    "\n",
    "def computeTFIDF(files,cDict):\n",
    "    IDTIF = np.zeros((len(files),len(cDict)))\n",
    "    myIDTIF=gensim.models.tfidfmodel.TfidfModel(dictionary=cDict)\n",
    "    c=0\n",
    "    for f in files:\n",
    "        IDTIF[c,:] = densify(myIDTIF[cDict.doc2bow(tokenizeFile(f))],len(cDict))\n",
    "        c+=1\n",
    "    return IDTIF\n",
    "\n",
    "def densify(vec,size):\n",
    "    dense = np.zeros(size)\n",
    "    for (i,v) in vec:\n",
    "        dense[i] = v\n",
    "    return dense\n",
    "\n",
    "def makeBOW(fileList, corpus):\n",
    "    bow = []\n",
    "    for file in fileList:\n",
    "        doc = tokenizeFile(file)\n",
    "        x = np.zeros(len(corpus))\n",
    "        xbow = corpus.doc2bow(doc)\n",
    "        for (wid, count) in xbow:\n",
    "            x[wid] = count\n",
    "        bow.append(x)\n",
    "    return np.array(bow)\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "def parallel_shuffle(A,B,seed = 1):\n",
    "    temp = list(zip(A,B))\n",
    "    random.Random(seed*seed).shuffle(temp)\n",
    "    A,B = zip(*temp)\n",
    "    return A,B\n",
    "\n",
    "#filepath is path of data in format used in the rest of the program\n",
    "#cc is current iteration of test files, start with cc=0 when calling, function returns next cc\n",
    "def orderData(filepath,cc):\n",
    "    files = None\n",
    "    testSize = 0.2\n",
    "    if type(filepath) is tuple:\n",
    "        (pathTrain,pathTest) = filepath\n",
    "        files,labels,index = loadDataWithTest(pathTrain,pathTest)\n",
    "        files = np.array(files)\n",
    "\n",
    "        cc = 5\n",
    "    else:\n",
    "        files,labels = loadDataSet(filepath)\n",
    "        # print (\"load\", filepath + \"/\" + str(cc) + 'perm.pickle')\n",
    "        # with open(filepath + \"/\" + str(cc) + 'perm.pickle', 'rb') as handle:\n",
    "        #        currentPerm = pickle.load(handle)\n",
    "        #        currentPerm.astype(int)\n",
    "        files = np.array(files)\n",
    "        # files = files[currentPerm]\n",
    "        # labels = labels[currentPerm]\n",
    "\n",
    "        files,labels = parallel_shuffle(files,labels,cc)\n",
    "        index = int(np.floor(len(labels)*(1-testSize)))\n",
    "        cc = cc + 1\n",
    "    trainFiles = files[0:index]\n",
    "    testFiles = files[index:]\n",
    "    trainLabels = labels[0:index]\n",
    "    testLabels = labels[index:]\n",
    "    return trainFiles, testFiles, trainLabels, testLabels, cc\n",
    "\n",
    "def run_CPWE_IDF_experiment(data, w2w, myDict, pars, savename):\n",
    "    print(\"CPWE IDF\")\n",
    "    (dataset,filepath) = data\n",
    "    cc = 0\n",
    "    accs = []\n",
    "    while cc < 5:\n",
    "        trainFiles, testFiles, trainLabels, testLabels, cc = orderData(filepath,cc)\n",
    "        ress = runCPWE_IDF(trainFiles, testFiles, trainLabels, testLabels, w2w, myDict)\n",
    "        acc, accMeans, best_k, kks,    preds, perc_preds, testLabels = ress\n",
    "        accs.append(allmeasures(preds, testLabels, perc_preds))\n",
    "        print (accs[-1])\n",
    "        with open(\"results/cpwe_idf\" + savename + str(cc) + '_results.pkl', 'wb') as f:\n",
    "                pickle.dump(ress, f)\n",
    "    print (\"------\")\n",
    "    print (\"mean\", np.mean(accs, axis=0))\n",
    "\n",
    "def runCPWE_IDF(trainFiles, testFiles, trainLabels, testLabels, w2wO, myDict, n_neighbors=-1, split = 0.3):\n",
    "    cvs = 1\n",
    "    random_states = np.random.permutation(cvs)\n",
    "    files = trainFiles\n",
    "    pars = np.arange(0.1,1.01,0.1)\n",
    "    pars[-1] = 1.0\n",
    "    kks = np.concatenate((np.array([2]),np.arange(1,20,2)))\n",
    "    print (\"(thresholds, k's)\", (pars,kks))\n",
    "    accs = np.zeros((len(pars),len(kks),cvs))\n",
    "    BOW = (makeBOW(files, myDict))\n",
    "    w2w = np.copy(w2wO)\n",
    "\n",
    "    #get frequencies\n",
    "    myIDTIF=gensim.models.tfidfmodel.TfidfModel(dictionary=myDict,normalize=False)\n",
    "    badidx = -1\n",
    "    for i in range(len(myDict)):\n",
    "        e = (myIDTIF[[(i,1)]])\n",
    "        if len(e) == 0:\n",
    "            print(i,e,myDict[i])\n",
    "            badidx = i\n",
    "\n",
    "    freq = myIDTIF[[(i,1) for i in range(len(myDict))]]\n",
    "    freq = [w for (i,w) in freq]\n",
    "    freq = np.array(freq)\n",
    "\n",
    "    if badidx > 0:\n",
    "        real_freq = np.zeros(len(freq)+1)\n",
    "        real_freq[:badidx] = freq[:badidx]\n",
    "        real_freq[badidx+1:] = freq[badidx:]\n",
    "        real_freq[badidx] = 0\n",
    "        freq = real_freq \n",
    "\n",
    "    freq = 1/(np.power(2,freq)/len(files))\n",
    "\n",
    "#     print(len(myDict), len(freq))\n",
    "\n",
    "    cc = 0\n",
    "    for splitted in pars:\n",
    "        kCounter = 0\n",
    "        if splitted > -0.0000001:\n",
    "            \n",
    "            w2w[w2w < splitted] = 0\n",
    "\n",
    "            w2wN = sklearn.preprocessing.normalize(w2w, axis=0)\n",
    "            w2wBOW = computeW2W(w2wN,BOW)\n",
    "            w2wBOW = csr_matrix(w2wBOW)\n",
    "            weights = np.dot(freq, w2wN)\n",
    "            weights = np.log2(len(files)/weights)\n",
    "            weights = csr_matrix(diags(weights))\n",
    "            w2wtfidf =  csr_matrix(w2wBOW * weights)\n",
    "            w2wtfidf = sklearn.preprocessing.normalize(w2wtfidf)\n",
    "\n",
    "            for k in kks:\n",
    "                for i in range(0,cvs):\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(w2wtfidf, trainLabels, test_size=split, random_state = random_states[i])\n",
    "                    clf = neighbors.KNeighborsClassifier(k,weights='distance',n_jobs=40)#, )\n",
    "                    clf.fit(X_train, y_train)\n",
    "\n",
    "                    acct = sklearn.metrics.accuracy_score(y_test, clf.predict(X_test))\n",
    "                    accs[cc,kCounter,i] = acct\n",
    "                kCounter = kCounter + 1\n",
    "\n",
    "            cc = cc + 1\n",
    "\n",
    "    accMeans = np.mean(accs,axis=2)\n",
    "    (bestSplit,best_k_index) = np.unravel_index(np.argmax(accMeans),accMeans.shape)\n",
    "\n",
    "    files = np.concatenate((trainFiles,testFiles))\n",
    "    BOW = (makeBOW(files, myDict))\n",
    "\n",
    "    splitted = pars[bestSplit]\n",
    "    best_k = kks[best_k_index]\n",
    "    print (\"best (threshold, k)\", (splitted,best_k))\n",
    "    w2w = np.copy(w2wO)\n",
    "    w2w[w2w < splitted] = 0\n",
    "    w2w = sklearn.preprocessing.normalize(w2w, axis=0)\n",
    "\n",
    "    w2wBOW = computeW2W(w2w,BOW)\n",
    "    w2wBOW = csr_matrix(w2wBOW)\n",
    "\n",
    "    weights = np.dot(freq, w2w)\n",
    "    weights = np.log2(len(files)/weights)\n",
    "    weights = diags(weights)\n",
    "\n",
    "    w2wtfidf =  (w2wBOW * weights)#[None,:]\n",
    "    w2wtfidf = sklearn.preprocessing.normalize(w2wtfidf)\n",
    "    w2wtfidf = w2wtfidf.todense()\n",
    "\n",
    "    X_train = w2wtfidf[0:len(trainFiles),:]\n",
    "    X_test = w2wtfidf[len(trainFiles):,:]\n",
    "\n",
    "    clf = neighbors.KNeighborsClassifier(best_k,weights='distance',n_jobs=40)\n",
    "    clf.fit(X_train,trainLabels)\n",
    "    preds = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(testLabels, preds)\n",
    "    perc_preds = clf.predict_proba(X_test)\n",
    "\n",
    "    print (\"#\", acc)\n",
    "    return acc, accMeans, best_k, kks,  preds, perc_preds, testLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling W2W constuction fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contructing Vocab and W2W for  amazon\n",
      "Dictionary(4784 unique tokens: ['anyth', 'bad', 'believ', 'book', 'bottl']...)\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "\n",
    "#     \"bbcsport\":\"bbcsport\",\n",
    "#     \"twitter\":\"twitter\",\n",
    "    \"amazon\":\"amazon\",\n",
    "#     \"20news\":(\"20news/20news-bydate-train\",\"20news/20news-bydate-test\"),\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "for dataset_name,path in datasets.items():\n",
    "    if(type(path) is tuple):\n",
    "        filepath = (\"datasets/\"+path[0],\"datasets/\"+path[1])\n",
    "    else:\n",
    "        filepath = \"datasets/\"+path\n",
    "    trainFiles, testFiles, trainLabels, testLabels, cc = orderData(filepath,0)\n",
    "    total_files = np.concatenate((trainFiles,testFiles)) \n",
    "    print(\"Contructing Vocab and W2W for \", dataset_name)\n",
    "\n",
    "    constructAndSave(total_files,dataset_name,True)\n",
    "    constructAndSave(total_files,dataset_name,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon\n",
      "CPWE IDF\n",
      "(thresholds, k's) (array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), array([ 2,  1,  3,  5,  7,  9, 11, 13, 15, 17, 19]))\n",
      "best (threshold, k) (0.30000000000000004, 15)\n",
      "# 0.855\n",
      "(0.9693203125000001, 0.9759449834612414, 0.855, 0.8552883701661254, 0.855)\n",
      "(thresholds, k's) (array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), array([ 2,  1,  3,  5,  7,  9, 11, 13, 15, 17, 19]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8cfc06eda1bc>:26: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best (threshold, k) (0.5, 19)\n",
      "# 0.85125\n",
      "(0.9720739583333333, 0.9741996044905635, 0.85125, 0.8512609253408746, 0.85125)\n",
      "(thresholds, k's) (array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), array([ 2,  1,  3,  5,  7,  9, 11, 13, 15, 17, 19]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8cfc06eda1bc>:26: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best (threshold, k) (0.5, 19)\n",
      "# 0.84375\n",
      "(0.9722359375, 0.9737183657739668, 0.84375, 0.8442080805447594, 0.84375)\n",
      "(thresholds, k's) (array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), array([ 2,  1,  3,  5,  7,  9, 11, 13, 15, 17, 19]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8cfc06eda1bc>:26: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best (threshold, k) (0.5, 17)\n",
      "# 0.8325\n",
      "(0.9674765625, 0.9720682066081467, 0.8325, 0.832385460870338, 0.8325)\n",
      "(thresholds, k's) (array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), array([ 2,  1,  3,  5,  7,  9, 11, 13, 15, 17, 19]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8cfc06eda1bc>:26: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best (threshold, k) (0.5, 15)\n",
      "# 0.8725\n",
      "(0.9720236979166667, 0.974337582982845, 0.8725, 0.8727534749596763, 0.8725)\n",
      "------\n",
      "mean [0.97062609 0.97405375 0.851      0.85117926 0.851     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8cfc06eda1bc>:26: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "\n",
    "#     \"bbcsport\":\"bbcsport\",\n",
    "#     \"twitter\":\"twitter\",\n",
    "    \"amazon\":\"amazon\",\n",
    "#     \"20news\":(\"20news/20news-bydate-train\",\"20news/20news-bydate-test\"),\n",
    "\n",
    "    }\n",
    "def test():\n",
    "    for dataset_name,path in datasets.items():\n",
    "        data = (dataset_name,\"datasets/\"+path)\n",
    "        print(dataset_name)\n",
    "        w2w = np.load(\"preprocessedFiles/\"+dataset_name+\"_Cosine.npy\",allow_pickle=True)\n",
    "#         norm = np.linalg.norm(w2w,ord=1)\n",
    "#         w2w = w2w/norm\n",
    "        with open(\"preprocessedFiles/\"+dataset_name+\".pickle\" , 'rb') as handle:\n",
    "            myDict = pickle.load(handle)\n",
    "        run_CPWE_IDF_experiment(data,w2w,myDict,None, dataset_name)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
